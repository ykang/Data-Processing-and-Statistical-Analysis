---
title: 'Lecture 2: Text Processing, Web Scraping and Text Mining'
author: "<br> <br >Yanfei Kang <br> yanfeikang@buaa.edu.cn"
date: "School of Economics and Management <br> Beihang University"
output:
  slidy_presentation:
    css: ../styles/ykstyle.css
    footer: 'Lecture 2: Text Processing, Web Scraping and Text Mining'
  html_document: default
logo: buaalogo.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE)
```

# In this lecture we are going to learn ...

- Text processing in **R**
- Web scraping in **R**
- Text mining in **R**

# Packages required in this lecture

```
install.packages(c("stringr", "rvest", "knitr", "jiebaR", "wordcloud2",
                  "tm", "slam" , "proxy", "topicmodels", "RColorBrewer"))
```

# {.alert}
<br>
<br>
Text Processing

# Read and write text data into R


```{r readlines}
# read text data into R
wmt.news <- readLines("WMTnews.txt") 
length(wmt.news)

# print the first news article without quotes
noquote(wmt.news[1])

# write text data into R
cat(wmt.news, file = "WMTnews.txt", sep = "\n")
```

- You may use other functions like `read.table()`, `write.table()` etc.


# Length of each article

- `nchar()`
- `stringr::str_length()`

```{r nchar}
# number of characters in each news article
nchar(wmt.news)
# library(stringr); str_length(wmt.news)
```

# Concatenate strings

#### Concatenate two strings

- `paste()`
- `stringr::str_c()`

```{r paste}
# concatenate characters 
paste('2015', '06-04', sep = '-')
paste('2015', c('06-04', '06-05'), sep = '-')
paste('2015', c('06-04', '06-05'), sep = '-', collapse = ' ')

# str_c() in stringr
library(stringr)
str_c('2015', '06-04', '00:00', sep = '-')

# frequently used in web scraping
paste('http://sou.zhaopin.com/jobs/searchresult.ashx?jl=åŒ—äº¬&kw=', 
      'é˜¿é‡Œå·´å·´', sep = '')
paste('http://sou.zhaopin.com/jobs/searchresult.ashx?jl=', 
      'ä¸Šæµ·', '&kw=', 'é˜¿é‡Œå·´å·´', sep = '')
```

#### Combine text and variable values
- `sprintf()` is a superior choice over paste.
```{r sprintf}
# combine text and variable values
comp <- 'é˜¿é‡Œå·´å·´'
job.location <- 'ä¸Šæµ·'
sprintf('http://sou.zhaopin.com/jobs/searchresult.ashx?jl=%s&kw=%s', 
        job.location, comp)
```

# Split strings

- `strsplit()`
- `stringr::str_split()`
```{r strsplit}
# split characters
dates <- c('2015-06-04', '2015-06-05')
strsplit(dates, "-")
strsplit('2015-06-04', '-')

# another way
library(stringr)
str_split(dates, '-')
str_split('2015-06-04', '-')
```


# Pattern Matching
```{r strmatch}
# search for matches
mySentences <- c('æ²ƒå°”ç›è¿˜ä¸å¾®ä¿¡è·¨ç•Œåˆä½œï¼Œé¡¾å®¢å¯é€šè¿‡æ²ƒå°”ç›å¾®ä¿¡æœåŠ¡å·çš„ä»˜æ¬¾åŠŸèƒ½åœ¨å®ä½“é—¨åº—ç§’ä»˜ä¹°å•ã€‚', 
                 'æ²ƒå°”ç›ç§»åŠ¨æ”¯ä»˜åº”ç”¨å·²ç»éƒ¨ç½²åœ¨å…¶å…¨ç¾4,600å®¶è¶…å¸‚ä¸­ã€‚')
grep('æ²ƒå°”ç›', mySentences)
grepl('æ²ƒå°”ç›', mySentences)
library(stringr); str_detect(mySentences, 'æ²ƒå°”ç›')
regexpr('æ²ƒå°”ç›', mySentences)
gregexpr('æ²ƒå°”ç›', mySentences)
```


# Pattern Replacement
```{r sub}
messySentences <- c('æ²ƒå°”ç›è¿˜ä¸å¾®ä¿¡   è·¨ç•Œåˆä½œï¼Œé¡¾å®¢å¯é€šè¿‡æ²ƒå°”ç›å¾®ä¿¡æœåŠ¡å·çš„ä»˜   æ¬¾åŠŸèƒ½åœ¨å®ä½“é—¨åº—ç§’ä»˜ä¹°å•ã€‚', 
                    'æ²ƒå°”ç›ç§»åŠ¨æ”¯ä»˜åº” ç”¨å·²ç»éƒ¨  ç½²åœ¨å…¶å…¨ç¾4,600å®¶è¶…å¸‚ä¸­ã€‚')

# patten replacement
# sub(pattern, replacement, x, ...)
sub(' ', '', messySentences)

# gsub(pattern, replacement, x, ...)
gsub(' ', '', messySentences)
```

# Extract substrings in a character vector

```{r substr}
# extract substrings: substr(x, start, stop)
x <- c('æœˆè–ªï¼š5000å…ƒ', 'æœˆè–ªï¼š8000å…ƒ')
substr(x,4,7)
```

# Your turn (1) ğŸ’“

1. Load text from the https://yanfei.site/docs/dpsa/BABAnews.txt and print it on screen. Text file contains some of the news of Alibaba.

2. How many paragraphs are there in the article?

3. Trim leading whitespaces of each paragraph (try `??trim`).

4. How many characters are there in the article?

5. Collapse paragraphs into one and display it on the screen (un-list it).

6. Does the text contain word 'æŠ€æœ¯æ¶æ„'?

7. Split the article into sentences (by periods).

8. Replace 'åŒ11' with 'åŒåä¸€'.

<!-- 8. Create a function that takes a string and returns it in reversed order by words (Hint: split the string according to a blank space ` '. Then count the number of components resulting from the splitting step. Re-arrenge them in reverse order and paste them in a single string.) -->


# Reference and further reading 

Please see [Text processing on Wiki](https://en.wikibooks.org/wiki/R_Programming/Text_Processing) for more details, examples, **R** packages and **R** functions used for text processing in **R**.



# {.alert}
<br>
<br>
Web Scraping


# We need data

- Data is messy!

<img src="./douban.png" width="1000px" height="780px" />

- But we want a tidy format of data!
    + rows == observations 
    + columns == attributes
    

| Movie         | Score          |Length (mins)   |Language         |
| ------------- |:-------------: |:-------------: | :-------------: | 
| çˆ±ä¹ä¹‹åŸ      | 8.4            | 128            |English          |
| çœ‹ä¸è§çš„å®¢äºº  | 8.7            | 106            |Spanish          |
| ...           | ...            |...             |  ...            | 

# Get familiar with the structure of a html (tags)

- When we do web scraping, we deal with html tags to find the path of the information we want to extract.

- A simple html source code: tree structure of html tags. HTML tags normally come in pairs.
    
```
<!DOCTYPE html>
<html>
  <title> My title
  </title>
  <body>
    <h1> My first heading </h1>
      <p> My first paragraph </p>
  </body>
</html>
```


#### XPath

- Path used to select nodes and info in html. Here are some examples of XPath expressions and their meanings:
    - **`/html/title`**: selects the `<title>` element of an HTML document 
    - **`//p`**: selects all the `<p>` elements


#  Another example
```
<html> 
  <head>
    <base href='http://example.com/' />
    <title>Example website</title> 
  </head>
  <body>
    <div id='images', class='img'>
      <a href='image1.html'>Name: My image 1 <br /><img src='image1_thumb.jpg'/></a> 
      <a href='image2.html'>Name: My image 2 <br /><img src='image2_thumb.jpg'/></a> 
      <a href='image3.html'>Name: My image 3 <br /><img src='image3_thumb.jpg'/></a> 
      <a href='image4.html'>Name: My image 4 <br /><img src='image4_thumb.jpg'/></a> 
      <a href='image5.html'>Name: My image 5 <br /><img src='image5_thumb.jpg'/></a>
    </div> 
    <div>
      <a href='img.html'> text <img src='img.jpg'/></a>
    </div>
  </body>
</html>
```

#### Xpath

- **`//div[@id="images"]`**: selects all the `<div>` elements which contain an attribute `id="images"`.
    - **`//div[@class="img"]`** 
    - **`//body/div`** 
    - Xpath is not unique. 
    - attributes like 'class' and 'id' are mostly used.
    
- **`//div[@id="images"]/a/`**: selects all the `<a>` elements inside the aforementioned element.


# Your turn (2) ğŸ’“


```
<td class="zwmc" style="width: 250px;">
  <div style="width: 224px;*width: 218px; _width:200px; float: left">
    <a style="font-weight: bold">é‡‘èåˆ†æå¸ˆ</a>
  </div>
</td>
```

> - Q: Please extract the `<a>` element from the source above.
> - A1: `//td[@class="zwmc"]/div/a`
> - A2: `//td[@class="zwmc"]//a`






# Let's do it!

Scrape job information from http://sou.zhaopin.com of jobs related to 'é˜¿é‡Œå·´å·´'.

- Inspect a web page (easily found in Chrome).
<center><img src="./zhilian.png" width="1000px" height="600px"/></center>

- Find the xpath for the elements you want to extract
    - E.g., xpath for job titles: `//td[@class="zwmc"]/div/a`.
<center><img src="./zhilian_element.png" width="1000px" height="600px"/></center>

# Your turn (3) ğŸ’“

Can you find xpath for companies, salaries, locations and links?


# *rvest* package in **R**


- `read_html()`: read html from its url.
- `html_nodes()`: select parts of an html document using XPath.
- These are done after using html_nodes().
    - `html_text()`:  extract all text inside the tag.
    - `html_attr()`: extract contents of a single attribute.
    - `html_attrs()`: extract all attributes. 

```{r zhilian0, cache=TRUE}
library(rvest)
url <- 'http://sou.zhaopin.com/jobs/searchresult.ashx?jl=åŒ—äº¬&kw=é˜¿é‡Œå·´å·´'
web <- read_html(url)
job_title_nodes <- html_nodes(web, xpath = '//td[@class="zwmc"]/div/a')
length(job_title_nodes)
job_title <- html_text(job_title_nodes)
job_title[1:5]
link <- html_attr(job_title_nodes, 'href')
link[1:5]
```

# Pipeable!

```
job_title_nodes <- html_nodes(web, xpath = '//td[@class="zwmc"]/div/a')
job_title <- html_text(job_title_nodes)
```
$\Downarrow$

```
job_title_nodes <- web %>%
  html_nodes(xpath = '//td[@class="zwmc"]/div/a')
job_title <- job_title_nodes %>% html_text()
```

$\Downarrow$

```
job_title <- web %>%
  html_nodes(xpath = '//td[@class="zwmc"]/div/a') %>%
  html_text()
```

# Let's extract as much information as we can

### Extract job title, company, salary, location and link

```{r zhilian1, cache=TRUE}
library(rvest)
url <- 'http://sou.zhaopin.com/jobs/searchresult.ashx?jl=åŒ—äº¬&kw=é˜¿é‡Œå·´å·´'
web <- read_html(url, encoding = "utf-8")
job_title <- web %>%
  html_nodes(xpath = '//td[@class="zwmc"]/div/a') %>%
  html_text()
link <- web %>%
  html_nodes(xpath = '//td[@class="zwmc"]/div/a') %>%
  html_attr('href')
# link = paste('[Link](', link, sep='')
# link <- paste(link, ')', sep='')
company <- web %>%
  html_nodes(xpath = '//td[@class="gsmc"]') %>%
  html_text()
salary <- web %>%
  html_nodes(xpath = '//td[@class="zwyx"]') %>%
  html_text()
location <- web %>%
  html_nodes(xpath = '//td[@class="gzdd"]') %>%
  html_text()
alibaba_jobs <- data.frame(job_title, company, salary, location, link)
library(knitr)
kable(head(alibaba_jobs), format = "html")
```



# Let's extract as much information as we can

### Extract more job details via its link

```{r zhilian2, cache=TRUE}
library(stringr)
get_job_detail <- function(link){
  link = as.character(link)
  web = read_html(link)
  experience = web %>%
    html_nodes(xpath = '//ul[@class="terminal-ul clearfix"]/li[5]/strong') %>%
    html_text()
  degree = web %>%
    html_nodes(xpath = '//ul[@class="terminal-ul clearfix"]/li[6]/strong') %>%
    html_text()
  number = web %>%
    html_nodes(xpath = '//ul[@class="terminal-ul clearfix"]/li[7]/strong') %>%
    html_text()
  description = web %>%
    html_nodes(xpath = '//div[@class="terminalpage-main clearfix"]/div/div[1]')%>%
    html_text()
  description = sub('æŸ¥çœ‹èŒä½åœ°å›¾', '', description)
  description = sub('å·¥ä½œåœ°å€ï¼š', '', description)
  description = sub('åŒ—äº¬', '', description)
  description = str_trim(description)
  link_details = data.frame(experience, degree, number, description)
  return(link_details)
}

job_details <- data.frame()
for (i in 1:nrow(alibaba_jobs)){
  job_details = rbind(job_details, get_job_detail(alibaba_jobs$link[i]))
}
alibaba_job_details <- cbind(alibaba_jobs, job_details)
kable(head(subset(alibaba_job_details, select = -description)), format = "html")
```



# Your turn (4) ğŸ’“

Please choose one exercise from the two below.

- Extract names, research interests, emails and links  of all [BUAA SEM Professors](http://sem.buaa.edu.cn/szdw/jsbd.htm)

- Extract at least 5 attributes of the movies listed on [Douban top 250](https://movie.douban.com/top250)



#{.alert}
<br>
<br>
Text Mining

# What is text mining?

## Raw human written text $\Rightarrow$ Structured information

- Word frequency and wordcloud
- Text clustering
- Topic models


# Job description text mining

```{r loaddata}
# read text data into R
alibaba_job_description <- as.character(alibaba_job_details$description)
```

# Preprocessing

## Word segment

```{r segment, cache=TRUE}
# word segment
library(jiebaR)
engine1 <- worker(stop_word = 'stopwords.txt')
new_user_word(engine1, c("æ–°æŠ€æœ¯", 'æ–°åª’ä½“'))
Words <-  c()
for(i in 1:length(alibaba_job_description)){
  Words <- c(Words,c(segment(alibaba_job_description[i], engine1)))
}
myStopwords <- c('å·¥ä½œ', 'åœ°å€', 'å…¬å¸', 'å²—ä½', 'æè¿°', 
                 'è´Ÿè´£', 'èŒä½', 'å¹´', 'ä¼˜å…ˆ', 'å…·å¤‡',
                 'ç†Ÿæ‚‰','ç›¸å…³')
Words <- Words[-which(Words %in% myStopwords)]
Words <- gsub("[0-9]+?",'', Words)
Words <- Words[nchar(Words) > 1]
Words <- toupper(Words)
head(Words)
```

# Word frequencies and wordcloud
```{r wordfreq}
# word frequencies
wordsNum <- table(unlist(Words))  
wordsNum <- sort(wordsNum, decreasing = TRUE)   
words.top150 <- head(wordsNum,150) 
library(RColorBrewer); colors <- brewer.pal(8,"Dark2")  
# Sys.setlocale("LC_CTYPE")
library(wordcloud2)
wordcloud2(words.top150, color = "random-dark", shape = 'circle', backgroundColor = 'white')
# letterCloud(words.top150, word= "A", color = "random-light",
#             backgroundColor = "white",size = 0.3)
```

# What tools are required?
```{r tools}
toolsNum <- wordsNum[substr(names(wordsNum),1,1)%in%  c("A","B","D","E","F","G","H","I","J","K","L","M","N","O","P","Q","R","S","T","U","V","W","X","Y","Z")]
tools.top50 <- head(toolsNum, 50) 
my.english.stopwords <- c('IN', 'OF', 'TO', 'AND', 'WITH', 'FOR', 
                          'THE', 'WORKING', 'WORK')
tools.top <- tools.top50[!(names(tools.top50) %in% my.english.stopwords)]
wordcloud2(tools.top, color = "random-dark", shape = 'circle', backgroundColor = 'white')
```

# Clustering
```{r clustering, cache=TRUE}
library(tm)
library(slam)
wmt.news <- readLines("WMTnews.txt") 
mixseg <- worker(stop_word = "stopwords.txt")
mixseg$bylines <- TRUE
word_list <- mixseg[wmt.news]
f <- function(x){
  x <- gsub("[0-9]+?",'', x)
  x[x == 'å·åº—'] <- '1å·åº—'
  x <- paste(x[nchar(x)>1], collapse = ' ')
  return(x)
  }
d.vec <- lapply(word_list,f)
corpus <- Corpus(VectorSource(d.vec))
myStopwords <- c('æ–°æµª', 'æ²ƒå°”ç›', 'å¹´',  'æœˆ', 'æ—¥','å…¬å¸', 'ä¸­å›½', 'æœ‰é™å…¬å¸')
stopwords <- readLines('stopwords.txt')
mycorpus <- tm_map(corpus,removeWords,c(stopwords, myStopwords))

control <- list(removePunctuation=T, 
             wordLengths = c(2, Inf), 
             # weighting = weightTfIdf,
             stopwords = c(stopwords, myStopwords))
d.dtm <- DocumentTermMatrix(mycorpus, control)
d.dtm <- d.dtm[row_sums(d.dtm)!=0, ]
d.dtm.sub <- removeSparseTerms(d.dtm, sparse=0.99)
library(proxy)
d.dist <- proxy::dist(as.matrix(d.dtm.sub), method='cosine')
fit <- hclust(d.dist, method="ward.D")
memb <- cutree(fit, k = 2)
plot(fit)
```

# Clustering
```{r clustering2, cache=TRUE}
findFreqTerms(d.dtm.sub[memb==1, ], 300)
findFreqTerms(d.dtm.sub[memb==2, ], 300)
```

# Topic models
```{r topicmodeling, cache=TRUE}
library(topicmodels)
ctm <- topicmodels::CTM(d.dtm.sub, k = 2)
terms(ctm, 2, 0.01)
```

# References 

- [Wikibooks on text processing](https://en.wikibooks.org/wiki/R_Programming/Text_Processing)
- [`rvest` package documentation](https://cran.r-project.org/web/packages/rvest/rvest.pdf)
- [`jiebaR` tutorial](https://jiebar.qinwf.com)

# Further readings

- Other **R** packages like `XML`, `RCurl` and `scrapeR` are also used for web scraping.
- Other packages for text mining can be found [here](https://cran.r-project.org/web/views/NaturalLanguageProcessing.html).
